{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLD_yYK_PtTO",
        "outputId": "3ecc406a-8b40-4cdb-867b-d2e53556e826"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.4 catboost==1.2\n",
        "!pip install kneed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ngboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3F8CmhNQMia"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import copy\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "import warnings\n",
        "\n",
        "\n",
        "\n",
        "from catboost import CatBoostRegressor\n",
        "from kneed import KneeLocator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o59DBWuHrn4a"
      },
      "source": [
        "# Inicializando os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Ks4qlbgAq1cW",
        "outputId": "6f3b5114-feeb-42cf-e7de-8ef0bc81e62f"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/Users/joao altarugio/Desktop/Projeto LaMav/data/Really important data/RefractiveIndex_clean.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4JYBR9X9mcz"
      },
      "outputs": [],
      "source": [
        "df = df.loc[:, (df != 0).any(axis=0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "UV5sGD3y9ow6",
        "outputId": "15f84a4e-3c51-4632-aeb4-98b3d44242bf"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelo 3 boostings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CV - Todos \n",
        "- Se quiser rodar todos os tune junto; O código abaixo não esta 100% otimizado em termos de paralelização como os tunes individuais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from multiprocessing import Pool\n",
        "import warnings\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def parallel_evaluate_objectives(param_jobs, n_jobs):\n",
        "    return Parallel(n_jobs=n_jobs)(\n",
        "        delayed(objective_global)(job) for job in param_jobs\n",
        "    )\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def evaluate_param_objective(args):\n",
        "    objective_func, param = args\n",
        "    return (objective_func(**param), param)\n",
        "\n",
        "\n",
        "#--- Métrica RRMSE ---\n",
        "def rrmse(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "    rrmse = rmse / np.std(y_true)\n",
        "    return rrmse\n",
        "\n",
        "#--- RandomSearch Customizado ---\n",
        "class RandomSearch:\n",
        "    def __init__(self, param_space, max_iter=2, n_jobs=2, random_state=None):\n",
        "        self.param_space = param_space\n",
        "        self.max_iter = max_iter\n",
        "        self.n_jobs = n_jobs\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "\n",
        "    def sample_params(self):\n",
        "        return {\n",
        "            key: self.rng.choice(values) for key, values in self.param_space.items()\n",
        "        }\n",
        "\n",
        "    def fmin(self, objective, **kwargs):\n",
        "        param_list = [self.sample_params() for _ in range(self.max_iter)]\n",
        "        param_jobs = [(objective, p) for p in param_list]\n",
        "\n",
        "        if self.n_jobs > 1:\n",
        "            with Pool(self.n_jobs) as pool:\n",
        "                results = pool.map(evaluate_param_objective, param_jobs)\n",
        "        else:\n",
        "            results = [evaluate_param_objective(job) for job in param_jobs]\n",
        "\n",
        "        results.sort(key=lambda x: x[0])  # menor erro\n",
        "        return results[0]  # (erro, melhores_params)\n",
        "\n",
        "\n",
        "#--- Parâmetros de busca para cada modelo ---\n",
        "\n",
        "num = 10  # Número de pontos a serem gerados em distribuições com linspace\n",
        "\n",
        "param_spaces = {\n",
        "    \"CatBoost\": {\n",
        "        \"iterations\": np.linspace(100, 1000, num=num, dtype=int).tolist(),\n",
        "        \"learning_rate\": np.round(np.linspace(0.01, 0.40, num=num), 4).tolist(),\n",
        "        \"depth\": list(range(1, 17)),  # [1, ..., 16]\n",
        "        \"l2_leaf_reg\": np.round(np.linspace(0.0, 7.0, num=num), 4).tolist(),\n",
        "        \"random_strength\": np.round(np.linspace(0.0, 1.0, num=num), 4).tolist(),\n",
        "        \"bagging_temperature\": np.round(np.linspace(0.0, 1.5, num=num), 4).tolist(),\n",
        "        \"border_count\": list(range(128, 255, 14)),\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        \"n_estimators\": np.linspace(100, 1000, num=num, dtype=int).tolist(),\n",
        "        \"learning_rate\": np.round(np.linspace(0.01, 0.40, num=num), 4).tolist(),\n",
        "        \"max_depth\": list(range(3, 17, 2)),  # geralmente não se usa profundidade 1-2 em XGB\n",
        "        \"subsample\": np.round(np.linspace(0.5, 1.0, num=6), 4).tolist(),\n",
        "        \"colsample_bytree\": np.round(np.linspace(0.5, 1.0, num=6), 4).tolist(),\n",
        "        \"reg_lambda\": np.round(np.linspace(0.0, 5.0, num=6), 4).tolist(),\n",
        "        \"reg_alpha\": np.round(np.linspace(0.0, 5.0, num=6), 4).tolist(),\n",
        "    },\n",
        "    \"LightGBM\": {\n",
        "        \"n_estimators\": np.linspace(100, 1000, num=num, dtype=int).tolist(),\n",
        "        \"learning_rate\": np.round(np.linspace(0.01, 0.40, num=num), 4).tolist(),\n",
        "        \"max_depth\": list(range(3, 17, 2)),\n",
        "        \"num_leaves\": list(range(20, 130, 11)),  # [20, 31, 42, ..., 129]\n",
        "        \"subsample\": np.round(np.linspace(0.5, 1.0, num=6), 4).tolist(),\n",
        "        \"colsample_bytree\": np.round(np.linspace(0.5, 1.0, num=6), 4).tolist(),\n",
        "        \"reg_lambda\": np.round(np.linspace(0.0, 5.0, num=6), 4).tolist(),\n",
        "        \"reg_alpha\": np.round(np.linspace(0.0, 5.0, num=6), 4).tolist(),\n",
        "    },\n",
        "    \"HistGradientBoosting\": {\n",
        "        \"max_iter\": np.linspace(100, 1000, num=num, dtype=int).tolist(),\n",
        "        \"learning_rate\": np.round(np.linspace(0.01, 0.40, num=num), 4).tolist(),\n",
        "        \"max_depth\": [None] + list(range(3, 17, 2)),  # incluir None para profundidade ilimitada\n",
        "        \"l2_regularization\": np.round(np.linspace(0.0, 5.0, num=num), 4).tolist(),\n",
        "        \"max_bins\": list(range(128, 255, 14)),  # similar ao CatBoost border_count\n",
        "        \"min_samples_leaf\": list(range(10, 51, 5)),  # controle do overfitting\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "#--- Modelos ---\n",
        "def get_model(name, params):\n",
        "    if name == \"XGBoost\":\n",
        "        return XGBRegressor(objective=\"reg:squarederror\", n_jobs=-1, **params)\n",
        "    elif name == \"HistGradientBoosting\":\n",
        "        return HistGradientBoostingRegressor(**params)\n",
        "    elif name == \"LightGBM\":\n",
        "        return LGBMRegressor(n_jobs=-1, **params)\n",
        "    elif name == \"CatBoost\":\n",
        "        return CatBoostRegressor(verbose=0, thread_count=-1, **params)\n",
        "    else:\n",
        "        raise ValueError(\"Modelo desconhecido\")\n",
        "\n",
        "#--- Tuning com validação cruzada ---\n",
        "def objective(params, model_name, X, y, kf):\n",
        "    scores = []\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = get_model(model_name, params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        scores.append(rrmse(y_val, y_pred))\n",
        "    return np.mean(scores)\n",
        "\n",
        "def objective_global(params_model_data):\n",
        "    params, model_name, X, y, kf = params_model_data\n",
        "    return objective(params, model_name, X, y, kf), params\n",
        "\n",
        "def tune_model(model_name, X, y):\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    resultados = pd.Series(index=X.index, dtype=float)\n",
        "\n",
        "    search = RandomSearch(param_spaces[model_name], max_iter=2, n_jobs=2)\n",
        "\n",
        "    # Prepara lista de amostras de hiperparâmetros + dados fixos para paralelizar\n",
        "    param_list = [search.sample_params() for _ in range(search.max_iter)]\n",
        "    param_jobs = [(p, model_name, X, y, kf) for p in param_list]\n",
        "\n",
        "    if search.n_jobs > 1:\n",
        "        results = parallel_evaluate_objectives(param_jobs, search.n_jobs)\n",
        "\n",
        "    else:\n",
        "        results = [objective_global(job) for job in param_jobs]\n",
        "\n",
        "    results.sort(key=lambda x: x[0])\n",
        "    best_score, best_params = results[0]\n",
        "\n",
        "    # Preencher predições finais com melhores parâmetros\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "        model = get_model(model_name, best_params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        resultados.iloc[val_idx] = y_pred\n",
        "\n",
        "    return resultados, best_params, best_score\n",
        "\n",
        "\n",
        "\n",
        "#--- Executar tuning para todos os modelos ---\n",
        "X = df.drop(columns=\"RefractiveIndex\")\n",
        "y = df[\"RefractiveIndex\"]\n",
        "resultados = pd.DataFrame(index=df.index)\n",
        "\n",
        "modelos = [\"XGBoost\", \"HistGradientBoosting\", \"LightGBM\", \"CatBoost\"]\n",
        "\n",
        "for nome in modelos:\n",
        "    print(f\"Tunando {nome}...\")\n",
        "    preds, best_params, score = tune_model(nome, X, y)\n",
        "    resultados[nome + \"_pred\"] = preds\n",
        "    print(f\"Melhores parâmetros de {nome}: {best_params}\")\n",
        "    print(f\"RRMSE: {score:.5f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parallel_evaluate_objectives(param_jobs, n_jobs):\n",
        "    return Parallel(n_jobs=n_jobs)(\n",
        "        delayed(objective_global)(job) for job in param_jobs\n",
        "    )\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def evaluate_param_objective(args):\n",
        "    objective_func, param = args\n",
        "    return (objective_func(**param), param)\n",
        "\n",
        "\n",
        "# --- Métrica RRMSE ---\n",
        "def rrmse(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "    rrmse = rmse / np.std(y_true)\n",
        "    return rrmse\n",
        "\n",
        "# --- RandomSearch Customizado ---\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def evaluate_param_objective(args):\n",
        "    objective_func, param = args\n",
        "    return (objective_func(**param), param)\n",
        "\n",
        "class RandomSearch:\n",
        "    def __init__(self, param_space, max_iter=200, n_jobs=-1, random_state=None):\n",
        "        self.param_space = param_space\n",
        "        self.max_iter = max_iter\n",
        "        self.n_jobs = n_jobs\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "\n",
        "    def sample_params(self):\n",
        "        return {\n",
        "            key: self.rng.choice(values) for key, values in self.param_space.items()\n",
        "        }\n",
        "\n",
        "    def fmin(self, objective, **kwargs):\n",
        "        param_list = [self.sample_params() for _ in range(self.max_iter)]\n",
        "        param_jobs = [(objective, p) for p in param_list]\n",
        "\n",
        "        if self.n_jobs > 1:\n",
        "            results = Parallel(n_jobs=self.n_jobs)(\n",
        "                delayed(evaluate_param_objective)(job) for job in param_jobs\n",
        "            )\n",
        "        else:\n",
        "            results = [evaluate_param_objective(job) for job in param_jobs]\n",
        "\n",
        "        results.sort(key=lambda x: x[0])  # menor erro\n",
        "        return results[0]\n",
        "\n",
        "\n",
        "\n",
        "# --- Parâmetros de busca para cada modelo ---\n",
        "\n",
        "num = 100  # Número de pontos a serem gerados em distribuições com linspace\n",
        "\n",
        "param_spaces = {\n",
        "    \"XGBoost\": {\n",
        "        \"n_estimators\": np.linspace(100, 1000, num=num, dtype=int).tolist(),\n",
        "        \"learning_rate\": np.round(np.linspace(0.01, 0.40, num=num), 4).tolist(),\n",
        "        \"max_depth\": list(range(3, 17, 2)),  # geralmente não se usa profundidade 1-2 em XGB\n",
        "        \"subsample\": np.round(np.linspace(0.5, 1.0, num=6), 4).tolist(),\n",
        "        \"colsample_bytree\": np.round(np.linspace(0.5, 1.0, num=6), 4).tolist(),\n",
        "        \"reg_lambda\": np.round(np.linspace(0.0, 5.0, num=6), 4).tolist(),\n",
        "        \"reg_alpha\": np.round(np.linspace(0.0, 5.0, num=6), 4).tolist(),\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# --- Modelos ---\n",
        "def get_model(name, params):\n",
        "    if name == \"XGBoost\":\n",
        "        return XGBRegressor(objective=\"reg:squarederror\", tree_method=\"hist\", n_jobs=-1, **params)\n",
        "    else:\n",
        "        raise ValueError(\"Modelo desconhecido\")\n",
        "\n",
        "# --- Tuning com validação cruzada ---\n",
        "def objective(params, model_name, X, y, kf):\n",
        "    scores = []\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = get_model(model_name, params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        scores.append(rrmse(y_val, y_pred))\n",
        "    return np.mean(scores)\n",
        "\n",
        "def objective_global(params_model_data):\n",
        "    params, model_name, X, y, kf = params_model_data\n",
        "    return objective(params, model_name, X, y, kf), params\n",
        "\n",
        "def tune_model(model_name, X, y):\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    resultados = pd.Series(index=X.index, dtype=float)\n",
        "\n",
        "    search = RandomSearch(param_spaces[model_name], max_iter=200, n_jobs=-1)\n",
        "\n",
        "    # Prepara lista de amostras de hiperparâmetros + dados fixos para paralelizar\n",
        "    param_list = [search.sample_params() for _ in range(search.max_iter)]\n",
        "    param_jobs = [(p, model_name, X, y, kf) for p in param_list]\n",
        "\n",
        "    if search.n_jobs > 1:\n",
        "        results = parallel_evaluate_objectives(param_jobs, search.n_jobs)\n",
        "\n",
        "    else:\n",
        "        results = [objective_global(job) for job in param_jobs]\n",
        "\n",
        "    results.sort(key=lambda x: x[0])\n",
        "    best_score, best_params = results[0]\n",
        "\n",
        "    # Preencher predições finais com melhores parâmetros\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "        model = get_model(model_name, best_params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        resultados.iloc[val_idx] = y_pred\n",
        "\n",
        "    return resultados, best_params, best_score\n",
        "\n",
        "\n",
        "\n",
        "# --- Executar tuning para todos os modelos ---\n",
        "X = df.drop(columns=\"RefractiveIndex\")\n",
        "y = df[\"RefractiveIndex\"]\n",
        "resultados = pd.DataFrame(index=df.index)\n",
        "\n",
        "modelos = [\"XGBoost\"]\n",
        "\n",
        "for nome in modelos:\n",
        "    print(f\"Tunando {nome}...\")\n",
        "    preds, best_params, score = tune_model(nome, X, y)\n",
        "    resultados[nome + \"_pred\"] = preds\n",
        "    print(f\"Melhores parâmetros de {nome}: {best_params}\")\n",
        "    print(f\"RRMSE: {score:.5f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parallel_evaluate_objectives(param_jobs, n_jobs):\n",
        "    return Parallel(n_jobs=n_jobs)(\n",
        "        delayed(objective_global)(job) for job in param_jobs\n",
        "    )\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def evaluate_param_objective(args):\n",
        "    objective_func, param = args\n",
        "    return (objective_func(**param), param)\n",
        "\n",
        "\n",
        "# --- Métrica RRMSE ---\n",
        "def rrmse(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "    rrmse = rmse / np.std(y_true)\n",
        "    return rrmse\n",
        "\n",
        "# --- RandomSearch Customizado ---\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def evaluate_param_objective(args):\n",
        "    objective_func, param = args\n",
        "    return (objective_func(**param), param)\n",
        "\n",
        "class RandomSearch:\n",
        "    def __init__(self, param_space, max_iter=200, n_jobs=-1, random_state=None):\n",
        "        self.param_space = param_space\n",
        "        self.max_iter = max_iter\n",
        "        self.n_jobs = n_jobs\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "\n",
        "    def sample_params(self):\n",
        "        return {\n",
        "            key: self.rng.choice(values) for key, values in self.param_space.items()\n",
        "        }\n",
        "\n",
        "    def fmin(self, objective, **kwargs):\n",
        "        param_list = [self.sample_params() for _ in range(self.max_iter)]\n",
        "        param_jobs = [(objective, p) for p in param_list]\n",
        "\n",
        "        if self.n_jobs > 1:\n",
        "            results = Parallel(n_jobs=self.n_jobs)(\n",
        "                delayed(evaluate_param_objective)(job) for job in param_jobs\n",
        "            )\n",
        "        else:\n",
        "            results = [evaluate_param_objective(job) for job in param_jobs]\n",
        "\n",
        "        results.sort(key=lambda x: x[0])  # menor erro\n",
        "        return results[0]\n",
        "\n",
        "\n",
        "\n",
        "# --- Parâmetros de busca para cada modelo ---\n",
        "\n",
        "num = 100  # Número de pontos a serem gerados em distribuições com linspace\n",
        "\n",
        "param_spaces = {\n",
        "    \"HistGradientBoosting\": {\n",
        "        \"max_iter\": np.linspace(100, 1000, num=num, dtype=int).tolist(),\n",
        "        \"learning_rate\": np.round(np.linspace(0.01, 0.40, num=num), 4).tolist(),\n",
        "        \"max_depth\": [None] + list(range(3, 17, 2)),  # incluir None para profundidade ilimitada\n",
        "        \"l2_regularization\": np.round(np.linspace(0.0, 5.0, num=num), 4).tolist(),\n",
        "        \"max_bins\": list(range(128, 255, 14)),  # similar ao CatBoost border_count\n",
        "        \"min_samples_leaf\": list(range(10, 51, 5)),  # controle do overfitting\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# --- Modelos ---\n",
        "def get_model(name, params):\n",
        "    if name == \"HistGradientBoosting\":\n",
        "        return HistGradientBoostingRegressor(**params)\n",
        "    else:\n",
        "        raise ValueError(\"Modelo desconhecido\")\n",
        "\n",
        "# --- Tuning com validação cruzada ---\n",
        "def objective(params, model_name, X, y, kf):\n",
        "    scores = []\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = get_model(model_name, params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        scores.append(rrmse(y_val, y_pred))\n",
        "    return np.mean(scores)\n",
        "\n",
        "def objective_global(params_model_data):\n",
        "    params, model_name, X, y, kf = params_model_data\n",
        "    return objective(params, model_name, X, y, kf), params\n",
        "\n",
        "def tune_model(model_name, X, y):\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    resultados = pd.Series(index=X.index, dtype=float)\n",
        "\n",
        "    search = RandomSearch(param_spaces[model_name], max_iter=200, n_jobs=-1)\n",
        "\n",
        "    # Prepara lista de amostras de hiperparâmetros + dados fixos para paralelizar\n",
        "    param_list = [search.sample_params() for _ in range(search.max_iter)]\n",
        "    param_jobs = [(p, model_name, X, y, kf) for p in param_list]\n",
        "\n",
        "    if search.n_jobs > 1:\n",
        "        results = parallel_evaluate_objectives(param_jobs, search.n_jobs)\n",
        "\n",
        "    else:\n",
        "        results = [objective_global(job) for job in param_jobs]\n",
        "\n",
        "    results.sort(key=lambda x: x[0])\n",
        "    best_score, best_params = results[0]\n",
        "\n",
        "    # Preencher predições finais com melhores parâmetros\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "        model = get_model(model_name, best_params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        resultados.iloc[val_idx] = y_pred\n",
        "\n",
        "    return resultados, best_params, best_score\n",
        "\n",
        "\n",
        "\n",
        "# --- Executar tuning para todos os modelos ---\n",
        "X = df.drop(columns=\"RefractiveIndex\")\n",
        "y = df[\"RefractiveIndex\"]\n",
        "resultados = pd.DataFrame(index=df.index)\n",
        "\n",
        "modelos = [\"HistGradientBoosting\"]\n",
        "\n",
        "for nome in modelos:\n",
        "    print(f\"Tunando {nome}...\")\n",
        "    preds, best_params, score = tune_model(nome, X, y)\n",
        "    resultados[nome + \"_pred\"] = preds\n",
        "    print(f\"Melhores parâmetros de {nome}: {best_params}\")\n",
        "    print(f\"RRMSE: {score:.5f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LightGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parallel_evaluate_objectives(param_jobs, n_jobs):\n",
        "    return Parallel(n_jobs=n_jobs)(\n",
        "        delayed(objective_global)(job) for job in param_jobs\n",
        "    )\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def evaluate_param_objective(args):\n",
        "    objective_func, param = args\n",
        "    return (objective_func(**param), param)\n",
        "\n",
        "\n",
        "# --- Métrica RRMSE ---\n",
        "def rrmse(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "    rrmse = rmse / np.std(y_true)\n",
        "    return rrmse\n",
        "\n",
        "# --- RandomSearch Customizado ---\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def evaluate_param_objective(args):\n",
        "    objective_func, param = args\n",
        "    return (objective_func(**param), param)\n",
        "\n",
        "class RandomSearch:\n",
        "    def __init__(self, param_space, max_iter=200, n_jobs=-1, random_state=None):\n",
        "        self.param_space = param_space\n",
        "        self.max_iter = max_iter\n",
        "        self.n_jobs = n_jobs\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "\n",
        "    def sample_params(self):\n",
        "        return {\n",
        "            key: self.rng.choice(values) for key, values in self.param_space.items()\n",
        "        }\n",
        "\n",
        "    def fmin(self, objective, **kwargs):\n",
        "        param_list = [self.sample_params() for _ in range(self.max_iter)]\n",
        "        param_jobs = [(objective, p) for p in param_list]\n",
        "\n",
        "        if self.n_jobs > 1:\n",
        "            results = Parallel(n_jobs=self.n_jobs)(\n",
        "                delayed(evaluate_param_objective)(job) for job in param_jobs\n",
        "            )\n",
        "        else:\n",
        "            results = [evaluate_param_objective(job) for job in param_jobs]\n",
        "\n",
        "        results.sort(key=lambda x: x[0])  # menor erro\n",
        "        return results[0]\n",
        "\n",
        "\n",
        "\n",
        "# --- Parâmetros de busca para cada modelo ---\n",
        "\n",
        "num = 100  # Número de pontos a serem gerados em distribuições com linspace\n",
        "\n",
        "param_spaces = {\n",
        "    \"LightGBM\": {\n",
        "        \"n_estimators\": np.linspace(100, 1000, num=num, dtype=int).tolist(),\n",
        "        \"learning_rate\": np.round(np.linspace(0.01, 0.40, num=num), 4).tolist(),\n",
        "        \"max_depth\": list(range(3, 17, 2)),\n",
        "        \"num_leaves\": list(range(20, 130, 11)),  # [20, 31, 42, ..., 129]\n",
        "        \"subsample\": np.round(np.linspace(0.5, 1.0, num=6), 4).tolist(),\n",
        "        \"colsample_bytree\": np.round(np.linspace(0.5, 1.0, num=6), 4).tolist(),\n",
        "        \"reg_lambda\": np.round(np.linspace(0.0, 5.0, num=6), 4).tolist(),\n",
        "        \"reg_alpha\": np.round(np.linspace(0.0, 5.0, num=6), 4).tolist(),\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# --- Modelos ---\n",
        "def get_model(name, params):\n",
        "    if name == \"LightGBM\":\n",
        "        return LGBMRegressor(n_jobs=-1, **params)\n",
        "    else:\n",
        "        raise ValueError(\"Modelo desconhecido\")\n",
        "\n",
        "# --- Tuning com validação cruzada ---\n",
        "def objective(params, model_name, X, y, kf):\n",
        "    scores = []\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = get_model(model_name, params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        scores.append(rrmse(y_val, y_pred))\n",
        "    return np.mean(scores)\n",
        "\n",
        "def objective_global(params_model_data):\n",
        "    params, model_name, X, y, kf = params_model_data\n",
        "    return objective(params, model_name, X, y, kf), params\n",
        "\n",
        "def tune_model(model_name, X, y):\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    resultados = pd.Series(index=X.index, dtype=float)\n",
        "\n",
        "    search = RandomSearch(param_spaces[model_name], max_iter=200, n_jobs=-1)\n",
        "\n",
        "    # Prepara lista de amostras de hiperparâmetros + dados fixos para paralelizar\n",
        "    param_list = [search.sample_params() for _ in range(search.max_iter)]\n",
        "    param_jobs = [(p, model_name, X, y, kf) for p in param_list]\n",
        "\n",
        "    if search.n_jobs > 1:\n",
        "        results = parallel_evaluate_objectives(param_jobs, search.n_jobs)\n",
        "\n",
        "    else:\n",
        "        results = [objective_global(job) for job in param_jobs]\n",
        "\n",
        "    results.sort(key=lambda x: x[0])\n",
        "    best_score, best_params = results[0]\n",
        "\n",
        "    # Preencher predições finais com melhores parâmetros\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "        model = get_model(model_name, best_params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        resultados.iloc[val_idx] = y_pred\n",
        "\n",
        "    return resultados, best_params, best_score\n",
        "\n",
        "\n",
        "\n",
        "# --- Executar tuning para todos os modelos ---\n",
        "X = df.drop(columns=\"RefractiveIndex\")\n",
        "y = df[\"RefractiveIndex\"]\n",
        "resultados = pd.DataFrame(index=df.index)\n",
        "\n",
        "modelos = [\"LightGBM\"]\n",
        "\n",
        "for nome in modelos:\n",
        "    print(f\"Tunando {nome}...\")\n",
        "    preds, best_params, score = tune_model(nome, X, y)\n",
        "    resultados[nome + \"_pred\"] = preds\n",
        "    print(f\"Melhores parâmetros de {nome}: {best_params}\")\n",
        "    print(f\"RRMSE: {score:.5f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CatB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 200\n",
        "def parallel_evaluate_objectives(param_jobs, n_jobs):\n",
        "    return Parallel(n_jobs=n_jobs)(\n",
        "        delayed(objective_global)(job) for job in param_jobs\n",
        "    )\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def evaluate_param_objective(args):\n",
        "    objective_func, param = args\n",
        "    return (objective_func(**param), param)\n",
        "\n",
        "\n",
        "# --- Métrica RRMSE ---\n",
        "def rrmse(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "    rrmse = rmse / np.std(y_true)\n",
        "    return rrmse\n",
        "\n",
        "# --- RandomSearch Customizado ---\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def evaluate_param_objective(args):\n",
        "    objective_func, param = args\n",
        "    return (objective_func(**param), param)\n",
        "\n",
        "class RandomSearch:\n",
        "    def __init__(self, param_space, max_iter=n, n_jobs=-1, random_state=None):\n",
        "        self.param_space = param_space\n",
        "        self.max_iter = max_iter\n",
        "        self.n_jobs = n_jobs\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "\n",
        "    def sample_params(self):\n",
        "        return {\n",
        "            key: self.rng.choice(values) for key, values in self.param_space.items()\n",
        "        }\n",
        "\n",
        "    def fmin(self, objective, **kwargs):\n",
        "        param_list = [self.sample_params() for _ in range(self.max_iter)]\n",
        "        param_jobs = [(objective, p) for p in param_list]\n",
        "\n",
        "        if self.n_jobs > 1:\n",
        "            results = Parallel(n_jobs=self.n_jobs)(\n",
        "                delayed(evaluate_param_objective)(job) for job in param_jobs\n",
        "            )\n",
        "        else:\n",
        "            results = [evaluate_param_objective(job) for job in param_jobs]\n",
        "\n",
        "        results.sort(key=lambda x: x[0])  # menor erro\n",
        "        return results[0]\n",
        "\n",
        "\n",
        "\n",
        "# --- Parâmetros de busca para cada modelo ---\n",
        "\n",
        "num = 100  # Número de pontos a serem gerados em distribuições com linspace\n",
        "\n",
        "param_spaces = {\n",
        "    \"CatBoost\": {\n",
        "        \"iterations\": np.linspace(100, 1000, num=num, dtype=int).tolist(),\n",
        "        \"learning_rate\": np.round(np.linspace(0.01, 0.40, num=num), 4).tolist(),\n",
        "        \"depth\": list(range(1, 15, 2)),\n",
        "        \"random_strength\": np.round(np.linspace(0.0, 1.0, num=num), 4).tolist(),\n",
        "        \"bagging_temperature\": np.round(np.linspace(0.0, 1.5, num=num), 4).tolist(),\n",
        "        \"border_count\": list(range(128, 255, 14)),\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# --- Modelos ---\n",
        "def get_model(name, params):\n",
        "    if name == \"CatBoost\":\n",
        "        return CatBoostRegressor(verbose=0, thread_count=-1, **params)\n",
        "    else:\n",
        "        raise ValueError(\"Modelo desconhecido\")\n",
        "\n",
        "# --- Tuning com validação cruzada ---\n",
        "def objective(params, model_name, X, y, kf):\n",
        "    scores = []\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = get_model(model_name, params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        scores.append(rrmse(y_val, y_pred))\n",
        "    return np.mean(scores)\n",
        "\n",
        "def objective_global(params_model_data):\n",
        "    params, model_name, X, y, kf = params_model_data\n",
        "    return objective(params, model_name, X, y, kf), params\n",
        "\n",
        "def tune_model(model_name, X, y):\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    resultados = pd.Series(index=X.index, dtype=float)\n",
        "\n",
        "    search = RandomSearch(param_spaces[model_name], max_iter=n, n_jobs=-1)\n",
        "\n",
        "    # Prepara lista de amostras de hiperparâmetros + dados fixos para paralelizar\n",
        "    param_list = [search.sample_params() for _ in range(search.max_iter)]\n",
        "    param_jobs = [(p, model_name, X, y, kf) for p in param_list]\n",
        "\n",
        "    best_score = np.inf\n",
        "    best_params = None\n",
        "\n",
        "    for i, job in enumerate(param_jobs):\n",
        "        if search.n_jobs > 1:\n",
        "            result = parallel_evaluate_objectives([job], search.n_jobs)[0]\n",
        "        else:\n",
        "            result = objective_global(job)\n",
        "\n",
        "        score, params = result\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "            print(f\"[Iteração {i+1}] Novo melhor RRMSE: {best_score:.5f}\")\n",
        "            print(f\"Hiperparâmetros: {best_params}\\n\")\n",
        "\n",
        "    # Preencher predições finais com melhores parâmetros encontrados\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "        model = get_model(model_name, best_params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        resultados.iloc[val_idx] = y_pred\n",
        "\n",
        "    return resultados, best_params, best_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Executar tuning para todos os modelos ---\n",
        "X = df.drop(columns=\"RefractiveIndex\")\n",
        "y = df[\"RefractiveIndex\"]\n",
        "resultados = pd.DataFrame(index=df.index)\n",
        "\n",
        "modelos = [\"CatBoost\"]\n",
        "\n",
        "for nome in modelos:\n",
        "    print(f\"Tunando {nome}...\")\n",
        "    preds, best_params, score = tune_model(nome, X, y)\n",
        "    resultados[nome + \"_pred\"] = preds\n",
        "    print(f\"Melhores parâmetros de {nome}: {best_params}\")\n",
        "    print(f\"RRMSE: {score:.5f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Boostings mais std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop(columns=['RefractiveIndex'])\n",
        "y = df['RefractiveIndex']\n",
        "\n",
        "# Inicializa o KFold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Define os modelos\n",
        "modelos = {\n",
        "    \"XGBoost\": XGBRegressor(n_estimators=581, learning_rate = 0.0376, max_depth = 13, subsample = 0.8, colsample_bytree = 1.0, reg_lambda = 5.0, reg_alpha = 0.0, random_state=42, tree_method=\"hist\", verbosity=0),\n",
        "    \"HistGradientBoosting\": HistGradientBoostingRegressor(max_iter=936, learning_rate = 0.1518, max_depth= 13, l2_regularization = 2.1212, max_bins = 198, min_samples_leaf = 10, random_state=42),\n",
        "    \"LightGBM\": LGBMRegressor(n_estimators=600, learning_rate =  0.0927, max_depth = 15, num_leaves = 108, subsample = 0.6, colsample_bytree = 0.7, reg_lambda = 3.0, reg_alpha = 0.0, random_state=42),\n",
        "    \"CatBoost\": CatBoostRegressor(iterations = 936, learning_rate =  0.337, random_strength = 0.1313, bagging_temperature = 0.5455, border_count = 254, random_state=42, verbose=0),\n",
        "}\n",
        "\n",
        "# Inicializa dicionário para armazenar as métricas de cada fold\n",
        "metricas_por_fold = {nome: {\"RD\": [], \"R2\": [], \"RMSE\": [], \"RRMSE\": []} for nome in modelos}\n",
        "\n",
        "# Loop de validação cruzada\n",
        "for train_idx, val_idx in kf.split(X):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "    \n",
        "    for nome, modelo in modelos.items():\n",
        "        modelo.fit(X_train, y_train)\n",
        "        y_pred = modelo.predict(X_val)\n",
        "\n",
        "        # Cálculo das métricas para este fold\n",
        "        rd = np.mean(np.abs(y_val - y_pred) / np.abs(y_val)) * 100\n",
        "        r2 = r2_score(y_val, y_pred)\n",
        "        rmse = np.sqrt(np.mean((y_val - y_pred) ** 2))\n",
        "        rrmse = rmse / np.std(y_val)\n",
        "\n",
        "        # Armazena as métricas\n",
        "        metricas_por_fold[nome][\"RD\"].append(rd)\n",
        "        metricas_por_fold[nome][\"R2\"].append(r2)\n",
        "        metricas_por_fold[nome][\"RMSE\"].append(rmse)\n",
        "        metricas_por_fold[nome][\"RRMSE\"].append(rrmse)\n",
        "\n",
        "# Agora calcula média e desvio padrão\n",
        "estatisticas_metricas = {}\n",
        "\n",
        "for nome, metricas in metricas_por_fold.items():\n",
        "    estatisticas_metricas[nome] = {\n",
        "        \"RD médio\": np.mean(metricas[\"RD\"]),\n",
        "        \"RD std\": np.std(metricas[\"RD\"], ddof=1),\n",
        "        \"R^2 médio\": np.mean(metricas[\"R2\"]),\n",
        "        \"R^2 std\": np.std(metricas[\"R2\"], ddof=1),\n",
        "        \"RMSE médio\": np.mean(metricas[\"RMSE\"]),\n",
        "        \"RMSE std\": np.std(metricas[\"RMSE\"], ddof=1),\n",
        "        \"RRMSE médio\": np.mean(metricas[\"RRMSE\"]),\n",
        "        \"RRMSE std\": np.std(metricas[\"RRMSE\"], ddof=1)\n",
        "    }\n",
        "\n",
        "# Converte para DataFrame para melhor visualização\n",
        "df_estatisticas = pd.DataFrame(estatisticas_metricas).T\n",
        "print(df_estatisticas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def notificar_telegram(token, chat_id, mensagem):\n",
        "    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n",
        "    payload = {\"chat_id\": chat_id, \"text\": mensagem}\n",
        "    response = requests.post(url, data=payload)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Erro ao enviar mensagem:\", response.text)\n",
        "\n",
        "# Exemplo de uso no final do seu script\n",
        "if __name__ == \"__main__\":\n",
        "    # ... seu código principal ...\n",
        "    \n",
        "    # Notifique o Telegram no final\n",
        "    TOKEN = \"7657381805:AAH_IsWMKewMROrhpqLCpU9zY8oX45He0MA\"\n",
        "    CHAT_ID = \"7178661110\"\n",
        "    MENSAGEM = \" -> Boosting terminou de rodar com sucesso!\"\n",
        "\n",
        "    notificar_telegram(TOKEN, CHAT_ID, MENSAGEM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rodando para dados limpos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "caminho_arquivo = \"/Users/joao altarugio/Desktop/Projeto LaMav/data/df_filtrado.pkl\"\n",
        "with open(caminho_arquivo, \"rb\") as f:\n",
        "   df_filtrado = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_filtrado.drop(columns=['RefractiveIndex'])\n",
        "y = df_filtrado['RefractiveIndex']\n",
        "\n",
        "# Inicializa o KFold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Define os modelos\n",
        "modelos = {\n",
        "    \"XGBoost\": XGBRegressor(n_estimators=581, learning_rate = 0.0376, max_depth = 13, subsample = 0.8, colsample_bytree = 1.0, reg_lambda = 5.0, reg_alpha = 0.0, random_state=42, tree_method=\"hist\", verbosity=0),\n",
        "    \"HistGradientBoosting\": HistGradientBoostingRegressor(max_iter=936, learning_rate = 0.1518, max_depth= 13, l2_regularization = 2.1212, max_bins = 198, min_samples_leaf = 10, random_state=42),\n",
        "    \"LightGBM\": LGBMRegressor(n_estimators=600, learning_rate =  0.0927, max_depth = 15, num_leaves = 108, subsample = 0.6, colsample_bytree = 0.7, reg_lambda = 3.0, reg_alpha = 0.0, random_state=42),\n",
        "    \"CatBoost\": CatBoostRegressor(iterations = 936, learning_rate =  0.337, random_strength = 0.1313, bagging_temperature = 0.5455, border_count = 254, random_state=42, verbose=0),\n",
        "}\n",
        "\n",
        "# Inicializa dicionário para armazenar as métricas de cada fold\n",
        "metricas_por_fold = {nome: {\"RD\": [], \"R2\": [], \"RMSE\": [], \"RRMSE\": []} for nome in modelos}\n",
        "\n",
        "# Loop de validação cruzada\n",
        "for train_idx, val_idx in kf.split(X):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "    \n",
        "    for nome, modelo in modelos.items():\n",
        "        modelo.fit(X_train, y_train)\n",
        "        y_pred = modelo.predict(X_val)\n",
        "\n",
        "        # Cálculo das métricas para este fold\n",
        "        rd = np.mean(np.abs(y_val - y_pred) / np.abs(y_val)) * 100\n",
        "        r2 = r2_score(y_val, y_pred)\n",
        "        rmse = np.sqrt(np.mean((y_val - y_pred) ** 2))\n",
        "        rrmse = rmse / np.std(y_val)\n",
        "\n",
        "        # Armazena as métricas\n",
        "        metricas_por_fold[nome][\"RD\"].append(rd)\n",
        "        metricas_por_fold[nome][\"R2\"].append(r2)\n",
        "        metricas_por_fold[nome][\"RMSE\"].append(rmse)\n",
        "        metricas_por_fold[nome][\"RRMSE\"].append(rrmse)\n",
        "\n",
        "# Agora calcula média e desvio padrão\n",
        "estatisticas_metricas = {}\n",
        "\n",
        "for nome, metricas in metricas_por_fold.items():\n",
        "    estatisticas_metricas[nome] = {\n",
        "        \"RD médio\": np.mean(metricas[\"RD\"]),\n",
        "        \"RD std\": np.std(metricas[\"RD\"], ddof=1),\n",
        "        \"R^2 médio\": np.mean(metricas[\"R2\"]),\n",
        "        \"R^2 std\": np.std(metricas[\"R2\"], ddof=1),\n",
        "        \"RMSE médio\": np.mean(metricas[\"RMSE\"]),\n",
        "        \"RMSE std\": np.std(metricas[\"RMSE\"], ddof=1),\n",
        "        \"RRMSE médio\": np.mean(metricas[\"RRMSE\"]),\n",
        "        \"RRMSE std\": np.std(metricas[\"RRMSE\"], ddof=1)\n",
        "    }\n",
        "\n",
        "# Converte para DataFrame para melhor visualização\n",
        "df_estatisticas = pd.DataFrame(estatisticas_metricas).T\n",
        "print(df_estatisticas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def notificar_telegram(token, chat_id, mensagem):\n",
        "    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n",
        "    payload = {\"chat_id\": chat_id, \"text\": mensagem}\n",
        "    response = requests.post(url, data=payload)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Erro ao enviar mensagem:\", response.text)\n",
        "\n",
        "# Exemplo de uso no final do seu script\n",
        "if __name__ == \"__main__\":\n",
        "    # ... seu código principal ...\n",
        "    \n",
        "    # Notifique o Telegram no final\n",
        "    TOKEN = \"7657381805:AAH_IsWMKewMROrhpqLCpU9zY8oX45He0MA\"\n",
        "    CHAT_ID = \"7178661110\"\n",
        "    MENSAGEM = \" -> Boosting terminou de rodar com sucesso!\"\n",
        "\n",
        "    notificar_telegram(TOKEN, CHAT_ID, MENSAGEM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
